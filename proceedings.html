<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>IROS 2024 workshop and competition onMulti-robot Percetion and Navigation in Logistics and Inspection | accepted papers & competiton result</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
	<!-- <link rel="stylesheet" href="assets/css/www-player.css" /> -->
	<noscript>
		<link rel="stylesheet" href="assets/css/noscript.css" />
	</noscript>
</head>

<body class="is-preload">

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Header -->
		<header id="header">
			<a href="index.html" class="logo">accepted papers & competiton result</a>
		</header>

		<!-- Nav -->
		<nav id="nav">
			<ul class="links">
				<li><a href="index.html">About</a></li>
				<li><a href="program.html">Competition</a></li>
				<li><a href="speakers.html">Speakers & Program</a></li>
				
				<li><a href="cfp.html">Call for Papers</a></li>
				<li class="active"><a href="proceedings.html">accepted papers & competiton result</a></li>
			</ul>
			<ul class="icons">
				<li><a href="mailto:shyuan@ntu.edu.sg?cc=xu0021ng@e.ntu.edu.sg" class="icon solid solo fa-envelope"><span
							class="label">Email</span></a></li>
			</ul>
		</nav>

		<div id="main">
			<div class="image-row">
				<div class="image-container2">
					<img src="images/scnd_blk_2.png" alt="Image 1">
				</div>
				<div class="image-container2">
					<img src="images/artboard.png" alt="Image 1">
				</div>
			</div>
			<style>
				td {
					border-bottom: 1px solid black;
				}
			</style>
			<section class="post">
				<div class="image-row">
					<div class="image-container2">
						<a href="https://www.ntu.edu.sg/" style="border-bottom: none;">
							<img src="images/ntu.png" alt="Image 1">
						</a>
					</div>
					<div class="image-container2">
						<a href="https://www.ntu.edu.sg/cartin" style="border-bottom: none;">
							<img src="images/cartin.png" alt="Image 1">
						</a>
						
					</div>
					<div class="image-container2">
						<a href="https://www.cuhk.edu.hk/chinese/index.html" style="border-bottom: none;">
							<img src="images/CUHK.png" alt="Image 1">
						</a>
					</div>
					<div class="image-container2">
						<a href="https://www.hkclr.hk/" style="border-bottom: none;">
							<img src="images/hklogo.jpg" alt="Image 1">
						</a>
					</div>
				</div>
				<!-- <p> To be released soon </p> -->


				<div class="leaderboard">
					<h1>leaderboard of Challenge</h1>
					<ul>
						<li>
							<span class="name">Rank</span>
							<span class="name">Team name</span>
							<span class="name">  Score  </span>
						</li>
						<li>
							<span class="name">1</span>
							<span class="name">XMU</span>
							<span class="name">16331.856</span>
						</li>
						<li>
							<span class="name">2</span>
							<span class="name">KIOS CoE</span>
							<span class="name">15280.395</span>
						</li>
						<li>
							<span class="name">3</span>
							<span class="name">HBW</span>
							<span class="name">11280.563</span>
						</li>
						<li>
							<span class="name">4</span>
							<span class="name">HKU-TRANSGP</span>
							<span class="name">11063.355</span>
						</li>
						<li>
							<span class="name">5</span>
							<span class="name">Explore-bots</span>
							<span class="name">17.092 </span>
						</li>
					</ul>
				</div>

				<br>
				
				<div class="accordion">
					<h1>accepted papers</h1>
					<div class="accordion-item">
						<button class="accordion-header"> 	
							Cooperative UAV Autonomy of Dronument: New Era in Cultural Heritage Preservation </button>
						<div class="accordion-content">
							<p><br><strong>Authors:</strong> Pavel Petracek, Vit Kratky, Matej Petrlik, Martin Saska <br>  <br>
								<strong>Abstract:</strong> Digital documentation of large interiors of historical buildings is an exhausting task since most of the areas of interest are beyond typical human reach. We advocate the use of autonomous teams of multi-rotor UAVs capable of agile control while perceiving the environment and planning in real time using on-board computation only. Autonomous UAVs speed up the documentation process by several orders of magnitude while allowing for a repeatable, accurate, and condition-independent solution capable of precise collision-free operation at great heights. The developed multi-robot approach allows for performing tasks requiring dynamic scene illumination in large-scale real-world scenarios, a process previously applicable only in small-scale laboratory-like conditions. Experimental analyses range from single-UAV imaging to specialized lighting techniques requiring accurate coordination of multiple UAVs. The system’s robustness is demonstrated in more than two hundred autonomous flights in fifteen historical monuments requiring superior safety while lacking access to external localization.
								<br> <br> <a href="papers/Cooperative_UAV_Autonomy_of_Dronument_New_Era_in_Cultural_Heritage_Preservation.pdf"><strong>PDF</strong> </a>
							</p>
						</div>
					</div>
					<div class="accordion-item">
						<button class="accordion-header"> 	
							Swarming Tight Interactions for Achieving Resistibility of Large Robotic Systems in Real-world Condition 
						</button>
						<div class="accordion-content">
							<p><br><strong>Authors:</strong> Jiri Horyna, Martin Saska <br>  <br>
								<strong>Abstract:</strong> This paper presents an autonomous swarm system designed to be an enabling technology for achieving resilience to both partial and complete dropouts of localization of individual vehicles in large teams. The challenge of creating a resilient swarm system across diverse mission types is closely tied to maintaining accurate state awareness, regardless of changing environmental conditions and external threats like jamming and spoofing of primary localization data. Leveraging purely relative measurements and onboard sensor data to ensure accurate state awareness despite intermittent localization failures is extremely important for enhancing security, resilience, and safety of cooperating systems including edge autonomous devices. By combining approaches increasing resilience during both partial and complete localization dropouts, the paper bridges the gap in enhancing the resilience of drone swarm operations, allowing them to adapt dynamically across a wide range of mission types. Herein, we introduce and discuss the description and results of these state-of-the-art distributed state estimation techniques, which significantly strengthen swarm system security against vulnerabilities posed by emerging threats.
								<br> <br> <a href="papers/Swarming_tight_interactions_for_achieving_resistibility_of_large_robotic.pdf"><strong>PDF</strong> </a>
							
							</p>
						</div>
					</div>
					<div class="accordion-item">
						<button class="accordion-header"> 	
							 	Cooperative Flight in Complex Environments using Heterogeneous UAVs and LiDAR-based Relative Localization  
						</button>
						<div class="accordion-content">
							<p><br><strong>Authors:</strong> Vaclav Pritzl, Petr Stepan, Martin Saska <br>  <br>
								<strong>Abstract:</strong> Existing research has achieved impressive results in giving the Unmanned Aerial Vehicles (UAVs) the ability to operate in challenging conditions thanks to the fusion of multiple sensory modalities and utilizing multiple UAVs, but many parts of the environment remain unreachable for current UAV approaches. Designing a cooperating UAV team capable of flying through constrained passages while simultaneously achieving accurate localization requires developing new methods for cooperative localization, navigation, multi-UAV path planning, and coordination. Our approach to multi-UAV cooperative flight utilizes relative localization using direct UAV detections from a 3D Light Detection and Ranging (LiDAR) and hierarchical team structure. A larger primary UAV (pUAV), equipped with 3D LiDAR, can quickly and accurately map large areas while having accurate localization robust to decreased visibility conditions. A miniature secondary UAV (sUAV), equipped with cameras, can fit into tight passages and explore spaces unreachable for larger UAVs. Combining UAVs of different sizes and sensory equipment effectively increases the operational space of the UAV team while increasing its robustness to challenging conditions. In this paper, we describe the methods enabling our approach, namely the LiDAR-based relative localization and relative pose estimation, cooperative UAV guiding, and multi-UAV exploration. The described approaches have been successfully deployed in multiple real-world experiments with all the algorithms running on board the UAVs with no external localization system nor external computational resources.
								<br> <br> <a href="papers/Cooperative_Flight_in_Complex_Environments_using_Heterogeneous.pdf"><strong>PDF</strong> </a>
							
							</p>
						</div>
					</div>

					<div class="accordion-item">
						<button class="accordion-header"> 	
							Modular Odometries Fusion with Online Targetless Extrinsic Calibration in a Loosely-Coupled SLAM Framework  
						</button>
						<div class="accordion-content">
							<p><br><strong>Authors:</strong> Jiaming Wu, YANG Lyu,Jiakai Gao <br>  <br>
								<strong>Abstract:</strong> To address the challenges of extrinsic calibration and motion fusion in modular odometry systems, this paper proposes a method to integrate both extrinsic calibration and fused localization into a unified system. First, we introduce a non-decoupled optimization approach for Online Targetless Extrinsic Calibration. Instead of decoupling the motion equation for extrinsic calibration, we formulate it as a graph optimization problem, which is solved iteratively using g2o. This method prevents rotational errors from propagating into translation errors by incorporating multiple motion constraints through additional edges in the graph. Next, we propose a Multi-State Loosely-Coupled SLAM Framework. The motion data from multiple sensors are transformed into a unified world coordinate system using the extrinsic parameters obtained from the calibration process. These transformed poses are then incorporated into the back-end optimization, where GTSAM is used to manage the factor graph and the constraints on the pose transformations. This framework effectively fuses the motion information from the modular odometry systems into a cohesive solution. Through experiments, we validate the effectiveness of the Online Targetless Extrinsic Calibration, achieving the required precision.
								<br> <br> <a href="papers/Modular_Odometries_Fusion_with_Online_Targetless_Extrinsic_Calibration_in_a_Loosely-Coupled_SLAM_Framework.pdf"><strong>PDF</strong> </a>
							
							</p>
						</div>
					</div>

					<div class="accordion-item">
						<button class="accordion-header"> 	
							CoCap: Coordinated Motion Capture for Multi-actor Scenes in Outdoor Environments
						</button>
						<div class="accordion-content">
							<p><br><strong>Authors:</strong> Aditya Rauniyar, Micah Corah, Sebastian Scherer <br>  <br>
								<strong>Abstract:</strong> Motion capture has become increasingly important, not only in computer animation but also in emerging fields like the metaverse and humanoid training. Capturing outdoor environments offers extended horizon scenes but introduces challenges with occlusions and obstacles. Recent approaches using multi-drone systems to capture multiple actor scenes often fail to account for multi-view consistency and reasoning across cameras in cluttered environments. Coordinated motion Capture (CoCap), inspired by Conflict-Based Search (CBS), addresses this issue by coordinating view planning to ensure multi-view reasoning during conflicts. In comparison to Sequential Planning and unconstrained methods, CoCap achieves performance similar to ideal, unconstrained cases. It also introduces a real-time, coverage-based heuristic, making it well-suited for dense environments.
								<br> <br> <a href="papers/CoCap_Coordinated_motion_Capture_for_multi-actor_scenes_in_outdoor_environments.pdf"><strong>PDF</strong> </a>
							
							</p>
						</div>
					</div>

					<div class="accordion-item">
						<button class="accordion-header"> 	
							Autonomous Multi-MAV Localization of Ionizing Radiation Sources Using Miniature Compton Cameras
						</button>
						<div class="accordion-content">
							<p><br><strong>Authors:</strong> Michal Werner, Tomáš Báča, Petr Štibinger,  Daniela Doubravová, Jaroslav Šolc, Jan Rusňák, Martin Saska<br>  <br>
								<strong>Abstract:</strong> A novel method for autonomous localization of multiple sources of gamma radiation using a group of Micro Aerial Vehicles (MAVs) is presented in this paper. The method utilizes an extremely lightweight (44 g) Compton camera MiniPIX TPX3. The compact size of the detector allows for deployment onboard safe and agile small-scale Unmanned Aerial Vehicles (UAVs). The proposed radiation mapping approach fuses measurements from multiple distributed Compton camera sensors to accurately estimate the positions of multiple radioactive sources in real time. Unlike commonly used intensity-based detectors, the Compton camera reconstructs the set of possible directions towards a radiation source from just a single ionizing particle. Therefore, the proposed approach can localize radiation sources without having to estimate the gradient of a radiation field or contour lines, which require longer measurements. The instant estimation is able to fully exploit the potential of highly mobile MAVs. The radiation mapping method is combined with an active search strategy, which coordinates the future actions of the MAVs in order to improve the quality of the estimate of the sources’ positions, as well as to explore the area of interest faster. The proposed solution is evaluated in simulation and real-world experiments with multiple Cesium-137 radiation sources.
								<br> <br> <a href="papers//Autonomous_Multi-MAV_Localization_of_Ionizing_Radiation_Sources.pdf"><strong>PDF</strong> </a>
							
							</p>
						</div>
					</div>

					<div class="accordion-item">
						<button class="accordion-header"> 	
							Integrating Online Learning and Connectivity Maintenance for Communication-Aware Multi-Robot Coordination
						</button>
						<div class="accordion-content">
							<p><br><strong>Authors:</strong> Yupeng Yang, Yiwei Lyu, Yanze Zhang,  Ian Gao, Wenhao Luo<br>  <br>
								<strong>Abstract:</strong> This paper proposes a novel data-driven control strategy for maintaining connectivity in networked multi-robot systems. Existing approaches often rely on a pre-determined communication model specifying whether pairwise robots can communicate given their relative distance to guide the connectivity-aware control design, which may not capture real-world communication conditions. To relax that assumption, we present the concept of Data-driven Connectivity Barrier Certificates, which utilize Control Barrier Functions (CBF) and Gaussian Processes (GP) to characterize the admissible control space for pairwise robots based on communication performance observed online. This allows robots to maintain a satisfying level of pairwise communication quality (measured by the received signal strength) while in motion. Then we propose a Data-driven Connectivity Maintenance (DCM) algorithm that combines (1) online learning of the communication signal strength and (2) a bi-level optimization-based control framework for the robot team to enforce global connectivity of the realistic multi-robot communication graph and minimally deviate from their task-related motions. We demonstrate the effectiveness of the algorithm through simulations with up to 20 robots.
								<br> <br> <a href="papers/Integrating_Online_Learning_and_Connectivity_Maintenance_for_Communication_Aware_Multi_Robot_Coordination.pdf"><strong>PDF</strong> </a>
							
							</p>
						</div>
					</div>

					<div class="accordion-item">
						<button class="accordion-header"> 	
							Multi-Agent Neural SLAM for Autonomous Robots
						</button>
						<div class="accordion-content">
							<p><br><strong>Authors:</strong> Tianchen Deng, Guole Shen, Xun Chen,  Hongming Shen, Yanbo Wang, Weidong Chen, Jingchuan Wang<br>  <br>
								<strong>Abstract:</strong> Neural implicit scene representations have recently shown promising results in dense visual SLAM. However, existing implicit SLAM algorithms are constrained to single-agent scenarios, and falls difficulty in large indoor scenes and long sequences, due to their single, global radiance field with finite capacity. To this end, we propose a novel multi-agent collaborative SLAM framework with joint scene representation, distributed camera tracking, intra-to-inter loop closure, and sub-map fusion. Specifically, we propose a distributed learning framework for multi-agent neural SLAM system to improve multi-agents cooperation and communication bandwidth efficiency. A novel intra-to-inter loop closure method is designed to achieve local (single-agent) and global map consistency. Our framework supports single-agent and multi-agents operation. Furthermore, to the best of our knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that provides both continuous-time trajectories groundtruth and high-accuracy 3D meshes groundtruth. To this end, we introduce the first real-world dataset covering both single-agent and multi-agent scenarios, ranging from small rooms to large-scale environments, with high-accuracy ground truth for 3D reconstruction meshes and continuous-time camera trajectory. This dataset can advance the development of the community. Experiments on<br/>various datasets demonstrate the superiority of the proposed method in both camera tracking and mapping. The dataset and code will open-source in Github.
								<br> <br> <a href="papers/Multi-Agent_Neural_SLAM_for_Autonomous_Robots.pdf"><strong>PDF</strong> </a>
							
							</p>
						</div>
					</div>


			
				<script src="assets/js/script.js"></script>

			</section>
		


			<!-- Post -->
			<!-- <section class="post">

				<style>
					td {
						border-bottom: 1px solid black;
					}
				</style>
				<div class="table-wrapper">
					<table>
						<thead>
							<tr>
								<th>Title</th>
								<th>Authors</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td><a href="papers/findik.pdf">Collaborative Adaptation: Learning to Recover from Unforeseen Malfunctions in Multi-Robot Teams</a></td>
								<td>Yasin Findik, Paul Robinette, Kshitij Jerath, Reza Ahmadzadeh</td>
							</tr>

							<tr>
								<td><a href="papers/bouzidi.pdf">Interaction-Aware Merging in Mixed Traffic with Integrated Game-theoretic Predictive Control and Inverse Differential Game</a></td>
								<td>Mohamed-Khalil Bouzidi, Ehsan Hashemi</td>
							</tr>

							<tr>
								<td><a href="papers/mahajan.pdf">Intent-Aware Autonomous Driving: A Case Study on Highway Merging Scenarios</a></td>
								<td>Nishtha Mahajan, Qi Zhang</td>
							</tr>

							<tr>
								<td><a href="papers/yu.pdf">Active Inverse Learning in Stackelberg Trajectory Games</a></td>
								<td>Yue Yu, Jacob Levy, Negar Mehr, David Fridovich-Keil, Ufuk Topcu</td>
							</tr>

							<tr>
								<td><a href="papers/kalaria.pdf">Towards Optimal Head-to-head Autonomous Racing with Curriculum Reinforcement Learning</a></td>
								<td>Dvij Rajesh Kalaria, Qin Lin, John M Dolan</td>
							</tr>

							<tr>
								<td><a href="papers/wu.pdf">iPLAN: Intent-Aware Planning in Heterogeneous Traffic via Distributed Multi-Agent Reinforcement Learning</a></td>
								<td>Xiyang Wu, Rohan Chandra, Tianrui Guan, Amrit Singh Bedi, Dinesh Manocha</td>
							</tr>

							<tr>
								<td><a href="papers/chandra.pdf">Decentralized Multi-Robot Social Navigation in Constrained Environments via Game-Theoretic Control Barrier Functions</a></td>
								<td>Rohan Chandra, Vrushabh Zinage, Efstathios Bakolas, Joydeep Biswas, Peter Stone</td>
							</tr>

							<tr>
								<td><a href="papers/sathi.pdf">Cooperation Dynamics in Multi-Agent Systems: Exploring Game-Theoretic Scenarios with Mean-Field Equilibria</a></td>
								<td>Sathi Vaigarai, Sabahat Shaik, Jaswanth Nidamanuri</td>
							</tr>

							<tr>
								<td><a href="papers/rowold.pdf">A Preview of Open-Loop and Feedback Nash Trajectories in Racing Scenarios</a></td>
								<td>Matthias Rowold</td>
							</tr>

							<tr>
								<td><a href="papers/guan.pdf">Zero-Sum Games Between Large-Population Heterogeneous Teams: A Reachability-based Analysis under Mean-Field Sharing</a></td>
								<td>Yue Guan, Mohammad Afshari, Panagiotis Tsiotras</td>
							</tr>

							<tr>
								<td><a href="papers/samak.pdf">Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem</a></td>
								<td>Tanmay Samak, Chinmay Samak, Venkat Krovi</td>
							</tr>

							<tr>
								<td><a href="papers/nagami.pdf">Learned Objectives for Game Theoretic Planning</a></td>
								<td>Keiko Nagami, Jaden Clark, Mac Schwager</td>
							</tr>

							<tr>
								<td><a href="papers/fay.pdf">Towards Risk Aware Racing Agents: Learning Adaptive Policies in Competitive Racing Games</a></td>
								<td>Kehlani Fay, Victor Shia</td>
							</tr>


						</tbody>
					</table>
				</div> -->

		</div>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/jquery.scrollex.min.js"></script>
		<script src="assets/js/jquery.scrolly.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>
		<!-- <script src="caric_image/IROS 2024 Cooperative Aerial Robots Inspection Challenge (CARIC@IROS24)_files/MathJax.js"></script> -->

</body>

</html>